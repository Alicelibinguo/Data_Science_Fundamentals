{"paragraphs":[{"title":"Objectives","text":"%md\n\nIn this notebook, you'll learn how to do the following:\n\n1. Load one or more files in numerous file formats\n2. Save one or more files in numerous file formats\n\n---\n\nIn lesson 1, you'll learn: \n\n1. Spark's Read API structure\n2. Spark's Write API structure\n\n---\n\nIn lesson 2, you'll learn how to: \n\n1. Read a single CSV file\n2. Save to CSV\n3. Save to JSON\n4. Save to Parquet\n4. Save to Orc\n\n---\n\nIn lesson 3, you'll learn how to read one or more:\n\n1. CSV files\n2. JSON files\n3. Parquet files\n4. Orc files","user":"anonymous","dateUpdated":"2018-04-26T16:01:04-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this notebook, you&rsquo;ll learn how to do the following:</p>\n<ol>\n  <li>Load one or more files in numerous file formats</li>\n  <li>Save one or more files in numerous file formats</li>\n</ol>\n<hr/>\n<p>In lesson 1, you&rsquo;ll learn: </p>\n<ol>\n  <li>Spark&rsquo;s Read API structure</li>\n  <li>Spark&rsquo;s Write API structure</li>\n</ol>\n<hr/>\n<p>In lesson 2, you&rsquo;ll learn how to: </p>\n<ol>\n  <li>Read a single CSV file</li>\n  <li>Save to CSV</li>\n  <li>Save to JSON</li>\n  <li>Save to Parquet</li>\n  <li>Save to Orc</li>\n</ol>\n<hr/>\n<p>In lesson 3, you&rsquo;ll learn how to read one or more:</p>\n<ol>\n  <li>CSV files</li>\n  <li>JSON files</li>\n  <li>Parquet files</li>\n  <li>Orc files</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1524773086341_656027447","id":"20180426-150446_106294536","dateCreated":"2018-04-26T15:04:46-0500","dateStarted":"2018-04-26T16:01:04-0500","dateFinished":"2018-04-26T16:01:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:250"},{"title":"Lesson 1: Overview","text":"%md\n\nIn this lesson, you will learn the basics of Spark's Read and Write API's.\n\nThis knowledge will give you the basic foundation you need to read and write files in countless formats. ","user":"anonymous","dateUpdated":"2018-04-26T15:54:20-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this lesson, you will learn the basics of Spark&rsquo;s Read and Write API&rsquo;s.</p>\n<p>This knowledge will give you the basic foundation you need to read and write files in countless formats.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524774073379_-1855822978","id":"20180426-152113_1100728709","dateCreated":"2018-04-26T15:21:13-0500","dateStarted":"2018-04-26T15:54:20-0500","dateFinished":"2018-04-26T15:54:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:251"},{"title":"Lesson 2: Read API","text":"%md\n\nSpark has an internal object called a _DataFrameReader_ that allows you to read from all manner of data sources. You access the _DataFrameReader_ simply by issuing the command *spark.read*.\n\nA _DataFrameReader_ has a number of methods. \n\n1. The first method is **format**. \n> It tells Spark which file format you want to read. Examples that you'll learn about in this notebook include \"csv\", \"json\", \"parquet\", and \"orc\". There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.\n\n2. The second method is **option**. \n> It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics upcoming.\n\n3. The third method is **schema**.\n> It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.\n\n4. The fourth method is **load**. \n> It tells Spark where to go get the data.\n\n---\n\n#### General Format\nThe general format looks like this: **spark.read().option().schema().load()**.\n\n---\n\n#### Read Modes\nSpark has three read modes that handle bad data in different ways.  \n1. **permissive** - sets all fields to *null*\n2. **dropMalformed** - drops row that contains bad data\n3. **failFast** - complete failure as soon as bad data encountered","user":"anonymous","dateUpdated":"2018-04-26T16:00:41-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark has an internal object called a <em>DataFrameReader</em> that allows you to read from all manner of data sources. You access the <em>DataFrameReader</em> simply by issuing the command <em>spark.read</em>.</p>\n<p>A <em>DataFrameReader</em> has a number of methods. </p>\n<ol>\n  <li>\n  <p>\n  <p>The first method is <strong>format</strong>. </p>\n  <blockquote>\n    <p>It tells Spark which file format you want to read. Examples that you&rsquo;ll learn about in this notebook include &ldquo;csv&rdquo;, &ldquo;json&rdquo;, &ldquo;parquet&rdquo;, and &ldquo;orc&rdquo;. There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.</p>\n  </blockquote></p></li>\n  <li>\n  <p>The second method is <strong>option</strong>. </p>\n  <blockquote>\n    <p>It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics upcoming.</p>\n  </blockquote></li>\n  <li>\n  <p>The third method is <strong>schema</strong>.</p>\n  <blockquote>\n    <p>It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.</p>\n  </blockquote></li>\n  <li>\n  <p>The fourth method is <strong>load</strong>. </p>\n  <blockquote>\n    <p>It tells Spark where to go get the data.</p>\n  </blockquote></li>\n</ol>\n<hr/>\n<h4>General Format</h4>\n<p>The general format looks like this: <strong>spark.read().option().schema().load()</strong>.</p>\n<hr/>\n<h4>Read Modes</h4>\n<p>Spark has three read modes that handle bad data in different ways.<br/>1. <strong>permissive</strong> - sets all fields to <em>null</em><br/>2. <strong>dropMalformed</strong> - drops row that contains bad data<br/>3. <strong>failFast</strong> - complete failure as soon as bad data encountered</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524774176000_729490417","id":"20180426-152256_1453077214","dateCreated":"2018-04-26T15:22:56-0500","dateStarted":"2018-04-26T16:00:41-0500","dateFinished":"2018-04-26T16:00:41-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:252"},{"title":"Lesson 2: Write API","text":"%md\n\nIn a similar vein, Spark's had an internal object called _DataFrameWriter_ that allows you to write to all manner of data types. You access the _DataFrameWriter_ simply by issuing the command *dataframe.write*.\n\nA _DataFrameReader_ has a number of methods. \n\n1. The first method is **format**. \n> It tells Spark which file format you want to read. Examples that you'll learn about in this notebook include \"csv\", \"json\", \"parquet\", and \"orc\". There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.\n\n2. The second method is **option**. \n> It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics upcoming.\n\n3. The third method is **schema**.\n> It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.\n\n4. The fourth method is **load**. \n> It tells Spark where to go get the data.\n\n","user":"anonymous","dateUpdated":"2018-04-26T16:00:21-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In a similar vein, Spark&rsquo;s had an internal object called <em>DataFrameWriter</em> that allows you to write to all manner of data types. You access the <em>DataFrameWriter</em> simply by issuing the command <em>dataframe.write</em>.</p>\n<p>A <em>DataFrameReader</em> has a number of methods. </p>\n<ol>\n  <li>\n  <p>\n  <p>The first method is <strong>format</strong>. </p>\n  <blockquote>\n    <p>It tells Spark which file format you want to read. Examples that you&rsquo;ll learn about in this notebook include &ldquo;csv&rdquo;, &ldquo;json&rdquo;, &ldquo;parquet&rdquo;, and &ldquo;orc&rdquo;. There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.</p>\n  </blockquote></p></li>\n  <li>\n  <p>The second method is <strong>option</strong>. </p>\n  <blockquote>\n    <p>It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics upcoming.</p>\n  </blockquote></li>\n  <li>\n  <p>The third method is <strong>schema</strong>.</p>\n  <blockquote>\n    <p>It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.</p>\n  </blockquote></li>\n  <li>\n  <p>The fourth method is <strong>load</strong>. </p>\n  <blockquote>\n    <p>It tells Spark where to go get the data.</p>\n  </blockquote></li>\n</ol>\n<h4>General Format</h4>\n<p>The general format looks like this: <strong>spark.read().option().schema().load()</strong>.</p>\n<h4>Read Modes</h4>\n<p>Spark has three read modes that handle bad data in different ways.<br/>1. <strong>permissive</strong> - sets all fields to <em>null</em><br/>2. <strong>dropMalformed</strong> - drops row that contains bad data<br/>3. <strong>failFast</strong> - complete failure as soon as bad data encountered</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524776084492_-283859397","id":"20180426-155444_215036148","dateCreated":"2018-04-26T15:54:44-0500","dateStarted":"2018-04-26T16:00:04-0500","dateFinished":"2018-04-26T16:00:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:253"},{"title":"1. Read a CSV","text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs.csv\"\n\ncsvFile = spark.read.format(\"csv\") \\\n            .option(\"mode\", \"FAILFAST\") \\\n            .option(\"inferSchema\", \"true\") \\\n            .load(path)","user":"anonymous","dateUpdated":"2018-04-26T15:08:17-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769068061_-199803462","id":"20180426-135748_589784033","dateCreated":"2018-04-26T13:57:48-0500","dateStarted":"2018-04-26T14:13:14-0500","dateFinished":"2018-04-26T14:14:38-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"title":"2. Save a CSV","text":"%pyspark\n\nsave_path =  \"/Users/davidziganto/data/higgs_multi\"\n\ncsvFile \\\n  .write \\\n  .format(\"csv\") \\\n  .mode(\"error\") \\\n  .save(save_path + \".csv\")","user":"anonymous","dateUpdated":"2018-04-26T15:08:25-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769345886_480046200","id":"20180426-140225_1116801505","dateCreated":"2018-04-26T14:02:25-0500","dateStarted":"2018-04-26T14:17:58-0500","dateFinished":"2018-04-26T14:19:13-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"title":"3. Read multiple CSV files","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2018-04-26T15:08:52-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524773305800_-2123310475","id":"20180426-150825_690864937","dateCreated":"2018-04-26T15:08:25-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"title":"Save JSON","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"json\") \\\n  .mode(\"error\") \\\n  .save(save_path + \".json\")","user":"anonymous","dateUpdated":"2018-04-26T14:19:37-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769759189_-1136317867","id":"20180426-140919_187520922","dateCreated":"2018-04-26T14:09:19-0500","dateStarted":"2018-04-26T14:19:37-0500","dateFinished":"2018-04-26T14:20:54-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"title":"Save Parquet","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"parquet\") \\\n  .mode(\"error\") \\\n  .save(save_path + \".parquet\")","user":"anonymous","dateUpdated":"2018-04-26T14:23:01-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524770246129_-1116754802","id":"20180426-141726_399591333","dateCreated":"2018-04-26T14:17:26-0500","dateStarted":"2018-04-26T14:23:01-0500","dateFinished":"2018-04-26T14:25:05-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"title":"Save Orc","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"orc\") \\\n  .mode(\"error\") \\\n  .save(save_path + \".orc\")","user":"anonymous","dateUpdated":"2018-04-26T14:25:56-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524770581296_2008032914","id":"20180426-142301_1499992378","dateCreated":"2018-04-26T14:23:01-0500","dateStarted":"2018-04-26T14:25:56-0500","dateFinished":"2018-04-26T14:29:34-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-04-26T14:57:37-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524772657235_818146296","id":"20180426-145737_199995115","dateCreated":"2018-04-26T14:57:37-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:260"}],"name":"1_Data_Sources","id":"2DD9HTP73","angularObjects":{"2CZWR1GGK:shared_process":[],"2CZ7856YP:shared_process":[],"2CX8JPHEF:shared_process":[],"2D186B2WU:shared_process":[],"2CY5HAT7D:shared_process":[],"2CZB79PJH:shared_process":[],"2CYFPK9UY:shared_process":[],"2CZPQ5RPQ:shared_process":[],"2CYH9N1YV:shared_process":[],"2CYCTCHVJ:shared_process":[],"2CXPH91QU:shared_process":[],"2CY7DV8BJ:shared_process":[],"2CZ846CKC:shared_process":[],"2CZ5JZ2DK:shared_process":[],"2CZ9C6XMZ:shared_process":[],"2D1TA5NMG:shared_process":[],"2CYEN6EZN:shared_process":[],"2CXA6C49M:shared_process":[],"2CY7EH4R3:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}