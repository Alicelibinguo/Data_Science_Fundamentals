{"paragraphs":[{"title":"Objectives","text":"%md\n\nIn this notebook, you'll learn how to do the following:\n\n1. Load one or more files in numerous file formats\n2. Save one or more files in numerous file formats\n\n---\n\nIn lesson 1, you'll learn: \n\n1. Spark's Read API structure\n2. Spark's Write API structure\n\n---\n\nIn lesson 2, you'll learn how to: \n\n1. Read a single CSV file\n2. Save to CSV\n3. Save to JSON\n4. Save to Parquet\n4. Save to Orc\n\n---\n\nIn lesson 3, you'll learn how to read one or more:\n\n1. CSV files\n2. JSON files\n3. Parquet files\n4. Orc files","user":"anonymous","dateUpdated":"2018-04-27T12:08:12-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this notebook, you&rsquo;ll learn how to do the following:</p>\n<ol>\n  <li>Load one or more files in numerous file formats</li>\n  <li>Save one or more files in numerous file formats</li>\n</ol>\n<hr/>\n<p>In lesson 1, you&rsquo;ll learn: </p>\n<ol>\n  <li>Spark&rsquo;s Read API structure</li>\n  <li>Spark&rsquo;s Write API structure</li>\n</ol>\n<hr/>\n<p>In lesson 2, you&rsquo;ll learn how to: </p>\n<ol>\n  <li>Read a single CSV file</li>\n  <li>Save to CSV</li>\n  <li>Save to JSON</li>\n  <li>Save to Parquet</li>\n  <li>Save to Orc</li>\n</ol>\n<hr/>\n<p>In lesson 3, you&rsquo;ll learn how to read one or more:</p>\n<ol>\n  <li>CSV files</li>\n  <li>JSON files</li>\n  <li>Parquet files</li>\n  <li>Orc files</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1524773086341_656027447","id":"20180426-150446_106294536","dateCreated":"2018-04-26T15:04:46-0500","dateStarted":"2018-04-27T12:08:12-0500","dateFinished":"2018-04-27T12:08:12-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6336"},{"text":"%md\n# Start of Lesson 1","user":"anonymous","dateUpdated":"2018-04-27T12:08:16-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Start of Lesson 1</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1524847112007_-1685733454","id":"20180427-113832_1945047941","dateCreated":"2018-04-27T11:38:32-0500","dateStarted":"2018-04-27T12:08:16-0500","dateFinished":"2018-04-27T12:08:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6337"},{"title":"Lesson 1-0: Overview","text":"%md\n\nIn this lesson, you will learn the basics of Spark's Read and Write API's.\n\nThis knowledge will give you the basic foundation you need to read and write files in countless formats. ","user":"anonymous","dateUpdated":"2018-04-27T12:08:17-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this lesson, you will learn the basics of Spark&rsquo;s Read and Write API&rsquo;s.</p>\n<p>This knowledge will give you the basic foundation you need to read and write files in countless formats.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524774073379_-1855822978","id":"20180426-152113_1100728709","dateCreated":"2018-04-26T15:21:13-0500","dateStarted":"2018-04-27T12:08:17-0500","dateFinished":"2018-04-27T12:08:17-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6338"},{"title":"Lesson 1-1: Read API","text":"%md\n\nSpark has an internal object called a _DataFrameReader_ that allows you to read from all manner of data sources. You access the _DataFrameReader_ simply by issuing the command *spark.read*.\n\nA _DataFrameReader_ has a number of methods. \n\n1. The first method is **format**. \n> It tells Spark which file format you want to read. Examples that you'll learn about in this notebook include \"csv\", \"json\", \"parquet\", and \"orc\". There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.\n\n2. The second method is **option**. \n> It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics forthcoming.\n\n3. The third method is **schema**.\n> It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.\n\n4. The fourth method is **load**. \n> It tells Spark where to go get the data.\n\n---\n\n#### General Format\nThe general format looks like this: **spark.read().option().schema().load()**.\n\n---\n\n#### Read Modes\nSpark has three read modes that handle bad data in different ways:  \n1. **permissive** - sets all fields to *null*\n2. **dropMalformed** - drops row that contains bad data\n3. **failFast** - complete failure as soon as bad data encountered","user":"anonymous","dateUpdated":"2018-04-27T12:08:22-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark has an internal object called a <em>DataFrameReader</em> that allows you to read from all manner of data sources. You access the <em>DataFrameReader</em> simply by issuing the command <em>spark.read</em>.</p>\n<p>A <em>DataFrameReader</em> has a number of methods. </p>\n<ol>\n  <li>\n  <p>\n  <p>The first method is <strong>format</strong>. </p>\n  <blockquote>\n    <p>It tells Spark which file format you want to read. Examples that you&rsquo;ll learn about in this notebook include &ldquo;csv&rdquo;, &ldquo;json&rdquo;, &ldquo;parquet&rdquo;, and &ldquo;orc&rdquo;. There are others including: plain text, MongoDB, AWS Redshift, XML, and many more.</p>\n  </blockquote></p></li>\n  <li>\n  <p>The second method is <strong>option</strong>. </p>\n  <blockquote>\n    <p>It tells Spark how to handle certain file types. Each file type has default parameters so this is optional. Specifics forthcoming.</p>\n  </blockquote></li>\n  <li>\n  <p>The third method is <strong>schema</strong>.</p>\n  <blockquote>\n    <p>It tells Spark how your data is structured. This is optional but highly desirable if you know the format, as it will speed up the read process.</p>\n  </blockquote></li>\n  <li>\n  <p>The fourth method is <strong>load</strong>. </p>\n  <blockquote>\n    <p>It tells Spark where to go get the data.</p>\n  </blockquote></li>\n</ol>\n<hr/>\n<h4>General Format</h4>\n<p>The general format looks like this: <strong>spark.read().option().schema().load()</strong>.</p>\n<hr/>\n<h4>Read Modes</h4>\n<p>Spark has three read modes that handle bad data in different ways:<br/>1. <strong>permissive</strong> - sets all fields to <em>null</em><br/>2. <strong>dropMalformed</strong> - drops row that contains bad data<br/>3. <strong>failFast</strong> - complete failure as soon as bad data encountered</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524774176000_729490417","id":"20180426-152256_1453077214","dateCreated":"2018-04-26T15:22:56-0500","dateStarted":"2018-04-27T12:08:22-0500","dateFinished":"2018-04-27T12:08:22-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6339"},{"title":"Lesson 1-2: Write API","text":"%md\n\nIn a similar vein, Spark has an internal object called _DataFrameWriter_ that allows you to write to all manner of data types. You access the _DataFrameWriter_ simply by issuing the command *dataframe.write*.\n\nA _DataFrameWriter_ has a number of methods. \n\n1. The first method is **format**. \n> It tells Spark which file format you want to write to. The default is parquet. This method is optional unless you are writing to a separate format.\n\n2. The second method is **option**. \n> It tells Spark what to do if data already resides at the specified location and how to handle certain file types. Specifics forthcoming.\n\n3. The third method is **partitionBy**.\n> Only available for file-based data writes. **This is an advanced concept and will not be covered in this tutorial**.\n\n4. The fourth method is **bucketBy**. \n> Only available for file-based data writes. **This is an advanced concept and will not be covered in this tutorial**.\n\n5. The firth method is **sortBy**. \n> Only available for file-based data writes. **This is an advanced concept and will not be covered in this tutorial**.\n\n6. The sixth method is **save**. \n> It tells Spark where to save the data.\n\n---\n\nNote: Oftentimes there is more than one way to accomplish some objective in Spark. For example, you can write data supplying the mode in the *option* method or you can directly call another method called *mode*. Both accomplish the same thing. Using *mode* may make your statement easier to follow.\n\n---\n\n#### General Format\nThe general format looks like this: **spark.write.format().mode().option().save()**\n\n---\n\n#### Save Modes\n\nSpark has four save modes that tell Spark how to react should it encounter already existing data:\n1. **append** - append data files to the list of already existing files\n2. **overwrite** - overwrites any data already existing\n3. **error** - returns an error and fails immediately if data already exists\n4. **ignore** - do nothing if data already exists","user":"anonymous","dateUpdated":"2018-04-27T12:08:25-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In a similar vein, Spark has an internal object called <em>DataFrameWriter</em> that allows you to write to all manner of data types. You access the <em>DataFrameWriter</em> simply by issuing the command <em>dataframe.write</em>.</p>\n<p>A <em>DataFrameWriter</em> has a number of methods. </p>\n<ol>\n  <li>\n  <p>\n  <p>The first method is <strong>format</strong>. </p>\n  <blockquote>\n    <p>It tells Spark which file format you want to write to. The default is parquet. This method is optional unless you are writing to a separate format.</p>\n  </blockquote></p></li>\n  <li>\n  <p>The second method is <strong>option</strong>. </p>\n  <blockquote>\n    <p>It tells Spark what to do if data already resides at the specified location and how to handle certain file types. Specifics forthcoming.</p>\n  </blockquote></li>\n  <li>\n  <p>The third method is <strong>partitionBy</strong>.</p>\n  <blockquote>\n    <p>Only available for file-based data writes. <strong>This is an advanced concept and will not be covered in this tutorial</strong>.</p>\n  </blockquote></li>\n  <li>\n  <p>The fourth method is <strong>bucketBy</strong>. </p>\n  <blockquote>\n    <p>Only available for file-based data writes. <strong>This is an advanced concept and will not be covered in this tutorial</strong>.</p>\n  </blockquote></li>\n  <li>\n  <p>The firth method is <strong>sortBy</strong>. </p>\n  <blockquote>\n    <p>Only available for file-based data writes. <strong>This is an advanced concept and will not be covered in this tutorial</strong>.</p>\n  </blockquote></li>\n  <li>\n  <p>The sixth method is <strong>save</strong>. </p>\n  <blockquote>\n    <p>It tells Spark where to save the data.</p>\n  </blockquote></li>\n</ol>\n<hr/>\n<p>Note: Oftentimes there is more than one way to accomplish some objective in Spark. For example, you can write data supplying the mode in the <em>option</em> method or you can directly call another method called <em>mode</em>. Both accomplish the same thing. Using <em>mode</em> may make your statement easier to follow.</p>\n<hr/>\n<h4>General Format</h4>\n<p>The general format looks like this: <strong>spark.write.format().mode().option().save()</strong></p>\n<hr/>\n<h4>Save Modes</h4>\n<p>Spark has four save modes that tell Spark how to react should it encounter already existing data:<br/>1. <strong>append</strong> - append data files to the list of already existing files<br/>2. <strong>overwrite</strong> - overwrites any data already existing<br/>3. <strong>error</strong> - returns an error and fails immediately if data already exists<br/>4. <strong>ignore</strong> - do nothing if data already exists</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524776084492_-283859397","id":"20180426-155444_215036148","dateCreated":"2018-04-26T15:54:44-0500","dateStarted":"2018-04-27T12:08:25-0500","dateFinished":"2018-04-27T12:08:25-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6340"},{"text":"%md\n# End of Lesson 1","user":"anonymous","dateUpdated":"2018-04-27T12:08:29-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>End of Lesson 1</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1524847091281_664282179","id":"20180427-113811_427977582","dateCreated":"2018-04-27T11:38:11-0500","dateStarted":"2018-04-27T12:08:29-0500","dateFinished":"2018-04-27T12:08:29-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6341"},{"text":"%md\n# Start of Lesson 2","user":"anonymous","dateUpdated":"2018-04-27T12:08:30-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Start of Lesson 2</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1524847123514_1149111766","id":"20180427-113843_1021088641","dateCreated":"2018-04-27T11:38:43-0500","dateStarted":"2018-04-27T12:08:30-0500","dateFinished":"2018-04-27T12:08:30-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6342"},{"title":"Lesson 2-1: Read a CSV","text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs.csv\"\n\ncsvFile = spark.read \\\n            .format(\"csv\") \\\n            .option(\"mode\", \"FAILFAST\") \\\n            .option(\"inferSchema\", \"true\") \\\n            .load(path)","user":"anonymous","dateUpdated":"2018-04-27T12:08:32-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769068061_-199803462","id":"20180426-135748_589784033","dateCreated":"2018-04-26T13:57:48-0500","dateStarted":"2018-04-27T12:08:32-0500","dateFinished":"2018-04-27T12:10:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6343"},{"text":"%md\n\nThe CSV reader has a number of available options. The following list provides some of the popular options, though it is not exhaustive. \n\nSee the docs for a full list.\n\n|Key|Values|Default|Description|\n|---|------|:-----:|-----------|\n|sep|any string character|,|single character separator|\n|header|true, false|false|first line is a header?|\n|escape|any string character| \\ |escape for other characters|\n|inferSchema|true, false|false|infer column types on read?|\n|nullValue|any string character|\"\"|how to represent null value|\n|nanValue|any string character|NaN|how to represent NaN or missing value|\n|compression|none, bzip2, deflate, gzip, lz4, snappy|none|compression to use on read|\n|dateFormat|see docs|yyyy-MM-dd|date format for data columns|","user":"anonymous","dateUpdated":"2018-04-27T12:48:03-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524850270768_-2084146504","id":"20180427-123110_1371379563","dateCreated":"2018-04-27T12:31:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7923","dateFinished":"2018-04-27T12:48:03-0500","dateStarted":"2018-04-27T12:48:03-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The CSV reader has a number of available options. The following list provides some of the popular options, though it is not exhaustive. </p>\n<p>See the docs for a full list.</p>\n<table>\n  <thead>\n    <tr>\n      <th>Key</th>\n      <th>Values</th>\n      <th align=\"center\">Default</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>sep</td>\n      <td>any string character</td>\n      <td align=\"center\">,</td>\n      <td>single character separator</td>\n    </tr>\n    <tr>\n      <td>header</td>\n      <td>true, false</td>\n      <td align=\"center\">false</td>\n      <td>first line is a header?</td>\n    </tr>\n    <tr>\n      <td>escape</td>\n      <td>any string character</td>\n      <td align=\"center\">\\ </td>\n      <td>escape for other characters</td>\n    </tr>\n    <tr>\n      <td>inferSchema</td>\n      <td>true, false</td>\n      <td align=\"center\">false</td>\n      <td>infer column types on read?</td>\n    </tr>\n    <tr>\n      <td>nullValue</td>\n      <td>any string character</td>\n      <td align=\"center\">&quot;&quot;</td>\n      <td>how to represent null value</td>\n    </tr>\n    <tr>\n      <td>nanValue</td>\n      <td>any string character</td>\n      <td align=\"center\">NaN</td>\n      <td>how to represent NaN or missing value</td>\n    </tr>\n    <tr>\n      <td>compression</td>\n      <td>none, bzip2, deflate, gzip, lz4, snappy</td>\n      <td align=\"center\">none</td>\n      <td>compression to use on read</td>\n    </tr>\n    <tr>\n      <td>dateFormat</td>\n      <td>see docs</td>\n      <td align=\"center\">yyyy-MM-dd</td>\n      <td>date format for data columns</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]}},{"title":"Lesson 2-2: Save a CSV","text":"%pyspark\n\nsave_path =  \"/Users/davidziganto/data/higgs_multi.csv\"\n\ncsvFile \\\n  .write \\\n  .format(\"csv\") \\\n  .mode(\"overwrite\") \\\n  .save(save_path + \".csv\")","user":"anonymous","dateUpdated":"2018-04-27T12:10:23-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769345886_480046200","id":"20180426-140225_1116801505","dateCreated":"2018-04-26T14:02:25-0500","dateStarted":"2018-04-27T12:10:23-0500","dateFinished":"2018-04-27T12:11:43-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6344"},{"text":"%md\n\nThe CSV writer has a number of available options. The following list provides some of the popular options, though it is not exhaustive. \n\nSee the docs for a full list.\n\n|Key|Values|Default|Description|\n|---|------|:-----:|-----------|\n|sep|any string character|,|single character separator|\n|header|true, false|false|first line is a header?|\n|nullValue|any string character|\"\"|how to represent null value|\n|nanValue|any string character|NaN|how to represent NaN or missing value|\n|compression|none, bzip2, deflate, gzip, lz4, snappy|none|compression to use on read|\n|dateFormat|see docs|yyyy-MM-dd|date format for data columns|","user":"anonymous","dateUpdated":"2018-04-27T12:48:05-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524850953262_-726156910","id":"20180427-124233_783501617","dateCreated":"2018-04-27T12:42:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8208","dateFinished":"2018-04-27T12:43:04-0500","dateStarted":"2018-04-27T12:43:04-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The CSV writer has a number of available options. The following list provides some of the popular options, though it is not exhaustive. </p>\n<p>See the docs for a full list.</p>\n<table>\n  <thead>\n    <tr>\n      <th>Key</th>\n      <th>Values</th>\n      <th align=\"center\">Default</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>sep</td>\n      <td>any string character</td>\n      <td align=\"center\">,</td>\n      <td>single character separator</td>\n    </tr>\n    <tr>\n      <td>header</td>\n      <td>true, false</td>\n      <td align=\"center\">false</td>\n      <td>first line is a header?</td>\n    </tr>\n    <tr>\n      <td>nullValue</td>\n      <td>any string character</td>\n      <td align=\"center\">&quot;&quot;</td>\n      <td>how to represent null value</td>\n    </tr>\n    <tr>\n      <td>nanValue</td>\n      <td>any string character</td>\n      <td align=\"center\">NaN</td>\n      <td>how to represent NaN or missing value</td>\n    </tr>\n    <tr>\n      <td>compression</td>\n      <td>none, bzip2, deflate, gzip, lz4, snappy</td>\n      <td align=\"center\">none</td>\n      <td>compression to use on read</td>\n    </tr>\n    <tr>\n      <td>dateFormat</td>\n      <td>see docs</td>\n      <td align=\"center\">yyyy-MM-dd</td>\n      <td>date format for data columns</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]}},{"title":"Lesson 2-3: Save JSON","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"json\") \\\n  .mode(\"overwrite\") \\\n  .save(save_path + \".json\")","user":"anonymous","dateUpdated":"2018-04-27T12:11:50-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524769759189_-1136317867","id":"20180426-140919_187520922","dateCreated":"2018-04-26T14:09:19-0500","dateStarted":"2018-04-27T12:11:50-0500","dateFinished":"2018-04-27T12:13:22-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6345"},{"text":"%md\n\nThe JSON writer has a number of available options. The following list provides some of the popular options, though it is not exhaustive. \n\nSee the docs for a full list.\n\n|Key|Values|Default|Description|\n|---|------|:-----:|-----------|\n|compression|none, bzip2, deflate, gzip, lz4, snappy|none|compression to use on read|\n|dateFormat|see docs|yyyy-MM-dd|date format for data columns|\n|allowComments|true, false|false|whether to ignore Java/C++ comments|\n|allowSingleQuotes|true, false|true|allow single and double quotes|","user":"anonymous","dateUpdated":"2018-04-27T12:56:12-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524851290329_-843663499","id":"20180427-124810_203600163","dateCreated":"2018-04-27T12:48:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8318","dateFinished":"2018-04-27T12:56:02-0500","dateStarted":"2018-04-27T12:56:02-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The JSON writer has a number of available options. The following list provides some of the popular options, though it is not exhaustive. </p>\n<p>See the docs for a full list.</p>\n<table>\n  <thead>\n    <tr>\n      <th>Key</th>\n      <th>Values</th>\n      <th align=\"center\">Default</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>compression</td>\n      <td>none, bzip2, deflate, gzip, lz4, snappy</td>\n      <td align=\"center\">none</td>\n      <td>compression to use on read</td>\n    </tr>\n    <tr>\n      <td>dateFormat</td>\n      <td>see docs</td>\n      <td align=\"center\">yyyy-MM-dd</td>\n      <td>date format for data columns</td>\n    </tr>\n    <tr>\n      <td>allowComments</td>\n      <td>true, false</td>\n      <td align=\"center\">false</td>\n      <td>whether to ignore Java/C++ comments</td>\n    </tr>\n    <tr>\n      <td>allowSingleQuotes</td>\n      <td>true, false</td>\n      <td align=\"center\">true</td>\n      <td>allow single and double quotes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]}},{"title":"Lesson 2-4: Save Parquet","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"parquet\") \\\n  .mode(\"overwrite\") \\\n  .save(save_path + \".parquet\")","user":"anonymous","dateUpdated":"2018-04-27T12:13:35-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524770246129_-1116754802","id":"20180426-141726_399591333","dateCreated":"2018-04-26T14:17:26-0500","dateStarted":"2018-04-27T12:13:35-0500","dateFinished":"2018-04-27T12:15:52-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6346"},{"text":"%md\n\nThe Parquet writer has only two available options: \n\n|Key|Values|Default|Description|\n|---|------|:-----:|-----------|\n|compression|none, bzip2, deflate, gzip, lz4, snappy|none|compression to use on read|\n|mergeSchema|true, false|default value of **spark.sql.parquet.mergeSchema**|enable/disable incremental adding of columns to newly written files in same table/folder|","user":"anonymous","dateUpdated":"2018-04-27T12:59:16-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524851767888_-1752333132","id":"20180427-125607_2112698703","dateCreated":"2018-04-27T12:56:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8405","dateFinished":"2018-04-27T12:59:02-0500","dateStarted":"2018-04-27T12:59:02-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The Parquet writer has only two available options: </p>\n<table>\n  <thead>\n    <tr>\n      <th>Key</th>\n      <th>Values</th>\n      <th align=\"center\">Default</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>compression</td>\n      <td>none, bzip2, deflate, gzip, lz4, snappy</td>\n      <td align=\"center\">none</td>\n      <td>compression to use on read</td>\n    </tr>\n    <tr>\n      <td>mergeSchema</td>\n      <td>true, false</td>\n      <td align=\"center\">default value of <strong>spark.sql.parquet.mergeSchema</strong></td>\n      <td>enable/disable incremental adding of columns to newly written files in same table/folder</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]}},{"title":"Lesson 2-5: Save Orc","text":"%pyspark\n\ncsvFile \\\n  .write \\\n  .format(\"orc\") \\\n  .mode(\"overwrite\") \\\n  .save(save_path + \".orc\")","user":"anonymous","dateUpdated":"2018-04-27T12:17:12-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524770581296_2008032914","id":"20180426-142301_1499992378","dateCreated":"2018-04-26T14:23:01-0500","dateStarted":"2018-04-27T12:17:13-0500","dateFinished":"2018-04-27T12:20:55-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6347"},{"text":"%md\n\nThe Orc writer has no available options. It is a columnar file format designed for Hadoop and optimized for Hive. \n\n---\n\n**Question:** Should I use Parquet or Orc?\n\n**Answer:** Use Parquet because it is optimized for Spark whereas Orc is not.","user":"anonymous","dateUpdated":"2018-04-27T13:00:55-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524851961917_1015428385","id":"20180427-125921_1986039205","dateCreated":"2018-04-27T12:59:21-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8502","dateFinished":"2018-04-27T13:00:55-0500","dateStarted":"2018-04-27T13:00:55-0500","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The Orc writer has no available options. It is a columnar file format designed for Hadoop and optimized for Hive. </p>\n<hr/>\n<p><strong>Question:</strong> Should I use Parquet or Orc?</p>\n<p><strong>Answer:</strong> Use Parquet because it is optimized for Spark whereas Orc is not.</p>\n</div>"}]}},{"text":"%md\n# End of Lesson 2","user":"anonymous","dateUpdated":"2018-04-27T12:21:09-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>End of Lesson 2</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1524847134337_931759286","id":"20180427-113854_239686787","dateCreated":"2018-04-27T11:38:54-0500","dateStarted":"2018-04-27T12:21:09-0500","dateFinished":"2018-04-27T12:21:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6348"},{"text":"%md\n# Start of Lesson 3","user":"anonymous","dateUpdated":"2018-04-27T12:21:10-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Start of Lesson 3</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1524847144902_-1358650917","id":"20180427-113904_1058481718","dateCreated":"2018-04-27T11:39:04-0500","dateStarted":"2018-04-27T12:21:10-0500","dateFinished":"2018-04-27T12:21:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6349"},{"title":"Lesson 3-1: Read multiple CSV files","text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs_multi.csv\"\n\ndf = spark.read \\\n        .format(\"csv\") \\\n        .option(\"mode\", \"FAILFAST\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .load(path)","user":"anonymous","dateUpdated":"2018-04-27T12:21:12-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524773305800_-2123310475","id":"20180426-150825_690864937","dateCreated":"2018-04-26T15:08:25-0500","dateStarted":"2018-04-27T12:21:12-0500","dateFinished":"2018-04-27T12:22:36-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6350"},{"title":"Lesson 3-2: Read multiple JSON files","text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs_multi.json\"\n\ndf = spark.read \\\n        .format(\"json\") \\\n        .option(\"mode\", \"FAILFAST\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .load(path)","user":"anonymous","dateUpdated":"2018-04-27T12:22:54-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524772657235_818146296","id":"20180426-145737_199995115","dateCreated":"2018-04-26T14:57:37-0500","dateStarted":"2018-04-27T12:22:54-0500","dateFinished":"2018-04-27T12:23:46-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6351"},{"title":"Lesson 3-3: Read multiple Parquet files","text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs_multi.parquet\"\n\ndf = spark.read \\\n        .format(\"parquet\") \\\n        .load(path)","user":"anonymous","dateUpdated":"2018-04-27T12:23:48-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524847471158_570755904","id":"20180427-114431_868675140","dateCreated":"2018-04-27T11:44:31-0500","dateStarted":"2018-04-27T12:23:48-0500","dateFinished":"2018-04-27T12:23:48-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6352"},{"text":"%pyspark\n\npath = \"/Users/davidziganto/data/higgs_multi.orc\"\n\ndf = spark.read \\\n        .format(\"orc\") \\\n        .load(path)","user":"anonymous","dateUpdated":"2018-04-27T12:23:58-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1524847751669_1996532100","id":"20180427-114911_2140651987","dateCreated":"2018-04-27T11:49:11-0500","dateStarted":"2018-04-27T12:23:58-0500","dateFinished":"2018-04-27T12:23:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6353"},{"text":"%md\n\nYou should now know how the Spark Read and Write APIs are structured. \n\nYou should also know how to read/write files from/to CSV, JSON, Parquet, and Orc. \n\nSpark has the ability to read from and write to countless formats including plain text, JDBC/ODBC, Cassandra, MongoDB, AWS Redshift, XML, and many more. We did not cover these in this tutorial. However, you now possess the fundamentals to get your projects moving. ","user":"anonymous","dateUpdated":"2018-04-27T13:06:18-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524847821400_675227756","id":"20180427-115021_2063681436","dateCreated":"2018-04-27T11:50:21-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6354","dateFinished":"2018-04-27T13:06:15-0500","dateStarted":"2018-04-27T13:06:15-0500","errorMessage":"","title":"Summary"},{"text":"%md\n","user":"anonymous","dateUpdated":"2018-04-27T13:06:15-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524852375387_546184680","id":"20180427-130615_660828289","dateCreated":"2018-04-27T13:06:15-0500","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8619"}],"name":"1_File_Based_Data_Sources","id":"2DD9HTP73","angularObjects":{"2CZWR1GGK:shared_process":[],"2CZ7856YP:shared_process":[],"2CX8JPHEF:shared_process":[],"2D186B2WU:shared_process":[],"2CY5HAT7D:shared_process":[],"2CZB79PJH:shared_process":[],"2CYFPK9UY:shared_process":[],"2CZPQ5RPQ:shared_process":[],"2CYH9N1YV:shared_process":[],"2CYCTCHVJ:shared_process":[],"2CXPH91QU:shared_process":[],"2CY7DV8BJ:shared_process":[],"2CZ846CKC:shared_process":[],"2CZ5JZ2DK:shared_process":[],"2CZ9C6XMZ:shared_process":[],"2D1TA5NMG:shared_process":[],"2CYEN6EZN:shared_process":[],"2CXA6C49M:shared_process":[],"2CY7EH4R3:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}